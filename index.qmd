---
title: "Predictive Modeling of Weather Station Data:"
subtitle: "Linear Regression vs. Graph Neural Network"
author: "Colby Fenters & Lilith Holland (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)

::: callout-important
**Remember:** Your goal is to make your audience understand and care
about your findings. By crafting a compelling story, you can effectively
communicate the value of your data science project.

Carefully read this template since it has instructions and tips to
writing!
:::

## Introduction

Accurate weather prediction is a crucial task with widespread
implications across agriculture, transportation, disaster preparedness,
and energy management. Traditional forecasting methods often rely on
statistical models or physics-based simulations, however, with the
advancement of graphical neural networks (GNN) we believe there is
potential in a more modern deep learning approach.

In this project, we explore the predictive power of a traditional linear
regression model and a GNN on real-world weather station data. Our aim
is to evaluate whether the GNN's ability to incorporate spatial
relationships between stations offers a measurable advantage over more
conventional techniques

The dataset consists of multiple weather stations located within the
same geographic region. Each station collects meteorological variables
over time, and can be represented as a node within a broader spatial
network. For the linear model baseline, a single model will be trained
using all stations' data aggregated per feature for each time step.

For the GNN the model will be trained on the entire network of stations,
where each node corresponds to a station and edges represent spatial
relationships. The graph is encoded via a dense adjacency matrix,
excluding self-connections. The GNN aims to leverage the inherent
spatial structure of the data, potentially capturing regional weather
patterns and inter-station dependencies that are invisible to
traditional models.

Our evaluation focuses on forecasting performance over the last 6-months
of the dataset. We asses how well each modelling approach predicts key
weather variables and investigate the conditions under which one model
may outperform the other.

## Methods

This section outlines the modeling approaches, data structure, and
training procedures used to compare the traditional linear model and the
GNN on weather station data.

### 1. Data Structure

The base dataset consists of 33 features, over 8 stations, with 96,408
hourly time-steps with intermittent sections of missing data. The key
features of interest are as follows with descriptions from
[@Herzmann_IEM_2023]:

| Feature | Description |
|------------|------------------------------------------------------------|
| station | Station identifier code (3/4 characters) |
| valid | Timestamp of the observation |
| lon | Longitude of station |
| lat | Latitude of station |
| elevation | Elevation of station |
| tmpf | Air temperature in Fahrenheit |
| relh | Relative humidity in percent |
| drct | Wind direction in degrees |
| sknt | Wind speed in knots |
| p01i | One hour precipitation for the period from the observation time to the time of the previous hourly precipitation reset |
| vsby | visibility in miles |

While the dataset has many other features included, the remaining
features are of little interest for the purposes of this project.

### 2. Cleaning Process

The cleaning process for the data was a multistage procedure that
started with:

1.  Filling in all missing time steps for all stations between the min
    and max dataset timestep

2.  Compressing the timesteps from 1hr intervals down to 6hr intervals

3.  Removing any features that were missing more than 10% of their
    values in more than 6 of the 8 stations.

4.  Remove any stations that are missing more than 2 years of valid data
    (some stations are newer than others)

5.  Perform correlation analysis on each feature by between nodes and
    within nodes

6.  Scale remaining features with appropriate scalers

7.  Transform remaining dataframe into an array of shape time, station,
    feature

### 3. Linear Model

The linear model is formulated as a time-series regression task. It uses
the feature information from the previous twenty-eight time steps (seven
days) to predict the feature values at the next time step. Each input
consists of an aggregation of each nodes feature for the five
meteorological features, resulting in a fixed length input vector per
prediction target.

$t_{+1}=\text{features}_t+\text{features}_{t-1}+...+\text{features}_{t-27}$

$\text{feature}_t=\text{tmpf, relh, sknt, drctsin, drctcos}$

The five input features are:

-   Temperature
-   Relative Humidity
-   Wind Speed
-   Wind Direction (represented as sin and cosine components)

### 4. GNN

The GNN is designed to capture spatiotemporal dependencies in the
weather station network. It is implemented using PyTorch and follows a
structure inspired by the Diffusion Convolutional Recurrant Neural
Network (DCRNN) architecture.

-   Architecture
    -   Input Format:
        -   Data is structured using the StaticGraphTemporalSignal
            format
        -   Data is split such that:
            -   Testing = last 6 months (730 6 hr time steps)
            -   Training = Head 80% of the remaining data
            -   Validation = Tail 20% of the remaining data

Each node represents a weather station and temporal sequences of node
features are used for prediction.

-   Layers:
    -   A DCRNN layer to capture spatial and temporal dependencies
    -   A ReLU activation function
    -   A Linear output layer for final prediction
    -   The full structure is:
        -   DCRN(140, 64) -\> ReLU -\> DCRN(64, 32) -\> ReLU -\>
            DCRN(32, 32) -\> ReLU -\> Linear(32, 1)
-   Training Configuration
    -   Optimizer
        -   Adam
-   Learning Rate:
    -   Base learning rate of 0.01 but will reduce by 0.1 at a plateau
        down until 1e-5
-   Epochs:
    -   Trained for a maximum of 100 epochs with an early exit on
        plateau callback

The model is trained to predict the temperature for the next time step
based on the preceding twnty-eight time steps, analogous to the linear
model.

## Analysis and Results

### Data Exploration and Visualization

#### 1. Source:

The data used in this project is source from the Iowa Environmental
Mesonet at the Iowa State University through [@Herzmann_IEM_2023], where
weather station data is aggregated from variable sources based on the
standards shown in [@faa_asos_manual]. For our specific use case we
decided to select data over a ten year period ranging from 2010 to 2020
for nine Kanasa nodes. These nodes were selected primarily due to their
proximity and location.

![](images/clipboard-2971954892.png){width="350"}

```{r, warning=FALSE, echo=FALSE}
reticulate::repl_python()
```

```{python, warning=FALSE, echo=FALSE}
# %pip install polars==1.22.0
# %pip install numpy
# %pip install pandas
# %pip install seaborn
# %pip install matplotlib
# %pip install pyarrow
# %pip install datetime
# %pip install contextily
# %pip install h3==3.7.7
# %pip install shapely
# %pip install geopandas
# %pip install scikit-learn
# %pip install tqdm
# %pip install hypothesis==6.135.26
# %pip install torch==2.7.0
# %pip install torch-scatter==2.1.2 -f https://data.pyg.org/whl/torch-2.7.0+cpu.html
# %pip install torch-sparse==0.6.18 -f https://data.pyg.org/whl/torch-2.7.0+cpu.html
# %pip install torch-geometric==2.6.1
# %pip install torch-geometric-temporal==0.56.2
```

```{python, warning=FALSE, echo=FALSE}
import os
import typing
import datetime
from pathlib import Path
import shutil

import numpy as np
import polars as pl
import seaborn as sns
import pandas as pd
from tqdm import tqdm
import geopy.distance
import scipy
import sklearn
import geopandas
import shapely
import matplotlib.pyplot as plt

import torch
import torch_geometric_temporal

import contextily as ctx
from h3 import h3
```

```{python, warning=FALSE, echo=FALSE}
start_date = datetime.datetime(2010, 1, 1, 0, 0)
end_date = datetime.datetime(2020, 12, 31, 0, 0)

seed = 3435
split_index = 730

pl.enable_string_cache()

data_path = r'kansas_asos_2010_2020.csv'
```

```{python, warning=FALSE, echo=FALSE}
metar_schema = {'station': pl.Categorical,
                'valid': pl.Datetime,
                'lon': pl.Float64,
                'lat': pl.Float64,
                'elevation': pl.Float64,
                'tmpf': pl.Float64,
                'dwpf': pl.Float64,
                'relh': pl.Float64,
                'drct': pl.Float64,
                'sknt': pl.Float64,
                'p01i': pl.Float64,
                'alti': pl.Float64,
                'mslp': pl.Float64,
                'vsby': pl.Float64,
                'gust': pl.Float64,
                'skyc1': pl.Categorical,
                'skyc2': pl.Categorical,
                'skyc3': pl.Categorical,
                'skyc4': pl.Categorical,
                'skyl1': pl.Float64,
                'skyl2': pl.Float64,
                'skyl3': pl.Float64,
                'skyl4': pl.Float64,
                'wxcodes': pl.String,
                'ice_accretion_1hr': pl.Float64,
                'ice_accretion_3hr': pl.Float64,
                'ice_accretion_6hr': pl.Float64,
                'peak_wind_gust': pl.Float64,
                'peak_wind_drct': pl.Float64,
                'peak_wind_time': pl.Datetime,
                'feel': pl.Float64,
                'metar': pl.String,
                'snowdepth': pl.Float64}
```

```{python, warning=FALSE, echo=FALSE}
asos_ldf = pl.scan_csv(data_path, null_values=['T', 'M', '///'], schema=metar_schema)\
    .drop('metar')\
    .with_columns(pl.col('valid').dt.round('1h').alias('valid'))
```

```{python, warning=FALSE, echo=FALSE}
full_date_series = np.arange(start_date, end_date, datetime.timedelta(hours=1))

asos_df = asos_ldf\
    .collect()\
    .select(pl.col('station', 'lat', 'lon', 'elevation'))\
    .unique()\
    .join(pl.DataFrame({'valid': full_date_series}), how='cross')\
    .join(asos_ldf.collect(), on=['station', 'valid'], how='left')\
    .with_columns(pl.col('valid').dt.round('6h').alias('valid'))\
    .drop('lat_right', 'lon_right', 'elevation_right')\
    .group_by(['station', 'valid'])\
    .mean()\
    .with_columns(pl.col(pl.Float64).cast(pl.Float32))
    
asos_df.head()
```

#### 2. EDA

Once the nine nodes were selected further analysis was required to both
reduce the volume of data as well as to ensure the quality of the data
as the source is very clear about the poor quality of the data provided.

This reduction was initially done by first finding which features were
missing no more than 10% of their values and then finding which nodes
for a year range were missing no more than 10% of their values. During
this step one node was dropped due to the fact that it was introduced
after 2020 and thus had no valid data for the date-range selected.

```{python, warning=FALSE, echo=FALSE}
potential_features = asos_ldf.drop('valid', 'station', 'lat', 'lon', 'elevation').collect_schema().names()
feature_list = []

for feature in potential_features:
    if not asos_df.select(pl.col(feature).is_null().all()).item():
        feature_list.append(feature)

stations_list = asos_df\
    .select(pl.col('station'))\
    .unique()\
    .to_series()\
    .to_list()
```

```{python, warning=FALSE, echo=FALSE}
def safe_index(item, lst):
    return item in lst
  
year_series = np.arange(start_date.year, end_date.year + 1, 1)
reduced_feature_df = pl.DataFrame(schema={**{'start_year': pl.Int64, 'end_year': pl.Int64, 'year_range': pl.Int64, 'year_label': pl.String, 'feature': pl.String},
                                          **{station: pl.Boolean for station in stations_list}
                                          })

for index, a in enumerate(year_series):
    for b in year_series[index:]:
        shifted_date_series = np.arange(datetime.date(a, 1, 1), datetime.date(b, 12, 31), datetime.timedelta(hours=6))
        year_filter_df = asos_df.filter(pl.col('valid').dt.year().is_between(a, b))
        for feature in feature_list:
            if (year_filter_df.select(pl.col(feature).null_count()) != year_filter_df.height).item():
                asos_pivot_df = year_filter_df\
                    .pivot(on='station', index='valid', values=feature, aggregate_function='mean')\
                    .drop('valid')
                valid_stations = [s.name for s in asos_pivot_df if not (s.null_count() > len(shifted_date_series)*0.1)]
                if len(valid_stations) >= 6:
                    if asos_pivot_df.var().select(pl.mean_horizontal(pl.all()).alias('mean')).item() > 5:
                        valid_stations.sort()
                        new_row = pl.DataFrame({**{'start_year': a,
                                                   'end_year': b,
                                                   'year_range': (b+1) - a,
                                                   'year_label': f'{a}-{b}',
                                                   'feature': feature},
                                                **{station: safe_index(station, valid_stations) for station in stations_list}
                                                })
                        reduced_feature_df = reduced_feature_df.vstack(new_row)
```

```{python, warning=FALSE, echo=FALSE}
features = reduced_feature_df.select(pl.col('feature')).unique().to_series().to_list()

n_cols = 3
n_rows = -(-len(features) // n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 4, n_rows * 3), constrained_layout=True,
                         sharex=True, sharey=True)
axes = axes.flatten()

for idx, feature in enumerate(features):
    plot_df = (
        reduced_feature_df
        .filter(pl.col('feature') == feature)
        .drop('feature', 'start_year', 'end_year', 'year_range')
        .to_pandas()
    )

    plot_df.index = plot_df['year_label'].astype(str)
    plot_df = plot_df.drop('year_label', axis=1)
    plot_df = plot_df.sort_index()

    ax = axes[idx]
    sns.heatmap(plot_df, cmap='magma', annot=True, cbar=False, ax=ax)

    ax.set_title(feature)

fig.suptitle("Station Participation by Feature and Year", fontsize=16)
fig.supxlabel("Stations")
fig.supylabel("Year")

plt.show()
```

With this visual a date range was selected foe 2018 to 2020 as this
range had the most valid features and stations while also being quite
recent. As this range was selected the ULS station was dropped resulting
in seven valid stations.

```{python, warning=FALSE, echo=FALSE}
df_t = reduced_feature_df\
    .filter(pl.col('year_label').eq('2018-2020'))\
    .drop('year_range', 'year_label', 'start_year', 'end_year')\
    .transpose(include_header=True)


new_headers = df_t.row(0)

df_t_no_header = df_t.slice(1, df_t.height - 1)

df_t_renamed = df_t_no_header.rename({old: str(new) for old, new in zip(df_t_no_header.columns, new_headers)})
```

```{python, warning=FALSE, echo=FALSE}
valid_features = [col for col in df_t_renamed.columns if col != "feature"]
valid_stations = df_t_renamed\
    .with_columns(pl.col(valid_features)\
                  .map_elements(lambda x: x == 'true', return_dtype=pl.Boolean))\
    .filter( pl.all_horizontal([pl.col(col) for col in valid_features]))\
    .select(pl.col('feature'))\
    .to_series()\
    .to_list()
```

```{python, warning=FALSE, echo=FALSE}
reduced_asos_df = asos_df\
    .filter(pl.col('valid').is_between(datetime.datetime(2018, 1, 1, 0, 0), datetime.datetime(2021, 1, 1, 0, 0)))\
    .filter(pl.col('station').is_in(valid_stations))\
    .select(pl.col(['station', 'valid', 'lat', 'lon', 'elevation'] + valid_features))\
    .sort(['valid', 'station'])\
    .with_columns([(pl.col('drct').map_elements(lambda x: np.sin(np.radians(x)), return_dtype=pl.Float64)).alias('drct_sin'),
                   (pl.col('drct').map_elements(lambda x: np.cos(np.radians(x)), return_dtype=pl.Float64)).alias('drct_cos')])\
    .drop('drct')

row_count, feature_count = reduced_asos_df.drop('station', 'valid', 'lat', 'lon', 'elevation').shape
valid_station = reduced_asos_df.select(pl.col('station')).head(7).to_series().to_list()
station_count = len(valid_station)
valid_features = reduced_asos_df.drop('station', 'valid', 'lat', 'lon', 'elevation').columns

station_matrix = reduced_asos_df.drop('station', 'valid', 'lat', 'lon', 'elevation').to_numpy().reshape(int(row_count/station_count), station_count, feature_count)
```

```{python, warning=FALSE, echo=FALSE}
reduced_asos_df
```

```{python, warning=FALSE, echo=FALSE}
# as this is true it means there are no time slices where all values are nan
not np.any(np.all(np.isnan(station_matrix), axis=(1, 2)))
```

```{python, warning=FALSE, echo=FALSE}
def compute_node_distance(node1, node2, inverse=False):
    coords_1 = (node1[1], node1[0])
    coords_2 = (node2[1], node2[0])
    horizontal_distance = geopy.distance.geodesic(coords_1, coords_2).km
    if inverse:
        try:
            horizontal_distance = 1/horizontal_distance
        except ZeroDivisionError:
            horizontal_distance = 0
    return horizontal_distance
```

```{python, warning=FALSE, echo=FALSE}
station_df = reduced_asos_df.select(pl.col(['station', 'lon', 'lat', 'elevation'])).unique().to_pandas()
grid_list = station_df.loc[:, ['lon', 'lat']].reset_index()[['lon', 'lat', 'index']].to_numpy().tolist()
grid_list = [sublist[:-1] + [int(sublist[-1])] for sublist in grid_list]


result_dict = {'index': [],
               'station': []}
result_dict = {**result_dict, **{str(i): [] for _, _, i in grid_list}}

for row_index, station in station_df.iterrows():
    for col_index, station2 in station_df.iterrows():
        result_dict[str(col_index)].append(compute_node_distance([station['lon'], station['lat']], [station2['lon'], station2['lat']], inverse=True))
    result_dict['station'].append(station['station'])
    result_dict['index'].append(row_index)

grid_map_df = pl.DataFrame(result_dict, schema={**{'station': pl.Categorical, 'index': pl.UInt64}, **{str(i): pl.Float64 for _, _, i in grid_list}})
```

```{python, warning=FALSE, echo=FALSE}
scaled_idistance = sklearn.preprocessing.minmax_scale(grid_map_df.drop('station', 'index'))
modified_adjacency_df = grid_map_df.select(pl.col(['station', 'index']))\
    .join(pl.DataFrame(scaled_idistance, schema=[str(i) for i, _ in enumerate(scaled_idistance)]).with_row_index(),
          on='index')
```

#### 3. Graph Creation

Once the valid date-range was selected and all features were reduced to
only those that could be imputed the graph needed to be generated.

As the number of nodes were low it made sense to generate a dense graph
without self referencing. This was simple done by looping over every
node and producing an adjacency matrix. The matrix was then weighted by
the inverse distance scaled with a minmax scaler to ensure the values
all ranged from 0 to 1.

```{python, warning=FALSE, echo=FALSE}
station_node_list = [shapely.geometry.Point(lon, lat) for lat, lon in station_df[['lat', 'lon']].to_numpy()]
stations_gdf = geopandas.GeoDataFrame(station_df.copy(), geometry=station_node_list, crs="EPSG:4326")

edge_list = []
n = len(station_node_list)
for i in range(n):
    for j in range(i + 1, n):
        edge_list.append(shapely.geometry.LineString([station_node_list[i], station_node_list[j]]))

edges_gdf = geopandas.GeoDataFrame(geometry=edge_list, crs="EPSG:4326")

fig, ax = plt.subplots(figsize=(18, 10))

edges_gdf.plot(ax=ax, color="xkcd:blue", linewidth=1, alpha=1)

stations_gdf.plot(ax=ax, color="xkcd:bright orange", markersize=10)

for i, row in stations_gdf.iterrows():
    ax.text(row.geometry.x, row.geometry.y, row['station'], fontsize=9)

ctx.add_basemap(ax, crs=stations_gdf.crs.to_string())
```

#### 4. Graph Imputation

Once the graph was created the data needed to be imputed both spatially
and temporally. This was done through a two step process where first the
data was imputed over the time slice only considering the spatial
information, and then a second time temporally only considering the
temporal information. While this process is not perfect it was able to
accurately impute data for the spatiotemporal graph structure with no
apparent issues as shown below.

```{python, warning=FALSE, echo=FALSE}
adj = modified_adjacency_df.drop('station', 'index').to_numpy()
adj = adj / adj.sum(axis=1, keepdims=True)

def spatial_impute(data, adj):
    imputed = data.copy()
    T, N, F = data.shape

    for t in range(T):
        for i in range(N):
            for f in range(F):
                if np.isnan(imputed[t, i, f]):
                    neighbor_vals = imputed[t, :, f]
                    weights = adj[i]
                    mask = ~np.isnan(neighbor_vals)

                    if mask.sum() > 0:
                        imputed[t, i, f] = np.dot(weights[mask], neighbor_vals[mask]) / weights[mask].sum()

    return imputed

def spatiotemporal_impute(data, adj):
    data = spatial_impute(data, adj)

    T, N, F = data.shape
    for i in range(N):
        for f in range(F):
            series = data[:, i, f]
            mask = ~np.isnan(series)
            if mask.sum() == 0:
                continue
            indices = np.arange(T)
            data[:, i, f] = np.interp(indices, indices[mask], series[mask])

    return data

data_imputed = spatiotemporal_impute(station_matrix, adj)
```

```{python, warning=FALSE, echo=FALSE}
pd.DataFrame(data_imputed[:200, :, 2]).plot(legend=False, subplots=True, figsize=(20, 8))
pd.DataFrame(station_matrix[:200, :, 2]).plot(legend=False, subplots=True, figsize=(20, 8))
```

#### 5. Correlation Analysis

Once the data was imputed and cleaned a Correlation Analysis was run to
see if any features should be removed due to strong correlation. This
analysis was done both between nodes and within the node. This analysis
found that dwpf and feel both had a strong correlation with the target
variable tmpf. This makes logical sense as both dewpoint and feels like
temperature quite literally use temperature to calculate their values.
As such these two features were removed.

```{python, warning=FALSE, echo=FALSE}
def node_correlation(data, node_number):
    T, N, F = data.shape

    corr_within_node = np.empty((N, F, F))

    for node in range(N):
        node_data = data[:, node, :]
        corr_within_node[node] = np.corrcoef(node_data, rowvar=False)

    corr_list = []
    for f_feature in range(F):
        inner_list = []
        for s_feature in range(F):
            if f_feature < s_feature:
                inner_list.append(np.nan)
            else:
                inner_list.append(float(corr_within_node[node_number, f_feature, s_feature]))
        corr_list.append(inner_list)
    return corr_list
  
def between_node_correlation(data, feature_number):
    feature_data = data[:, :, feature_number]

    corr_between_nodes = np.corrcoef(feature_data.T)
    corr_between_nodes[np.triu_indices(corr_between_nodes.shape[0], 1)] = np.nan

    return corr_between_nodes
```

```{python, warning=FALSE, echo=FALSE}
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.flatten()

for i in range(data_imputed.shape[1]):
    corr_matrix = pd.DataFrame(
        node_correlation(data_imputed, i),
        columns=valid_features,
        index=valid_features
    )
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[i])
    axes[i].set_title(f'{valid_station[i]} Correlation')

plt.tight_layout()
plt.show()
```

Following the within node correlation, features were then compared
between nodes which showed if it made any sense to consider the spatial
relationship of the data. As shown below there is a very strong
correlation between all nodes irregardless of feature.

```{python, warning=FALSE, echo=FALSE}
fig, axes = plt.subplots(2, 4, figsize=(20, 10))
axes = axes.flatten()

for i in range(data_imputed.shape[2]):
    corr_matrix = pd.DataFrame(
        between_node_correlation(data_imputed, i),
        columns=valid_station,
        index=valid_station
    )
    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[i])
    axes[i].set_title(f'Between-Node Correlation: {valid_features[i]}')

plt.tight_layout()
plt.show()
```

#### 6. Scaling

Following all of the above steps the final dataset was reduced to 5
features, over 7 stations with 4381 6hr time steps. This dataset was
then scaled through the use of sklearns robust scaler.

```{python, warning=FALSE, echo=FALSE}
T, N, F = data_imputed.shape
imputed_df = pl.from_numpy(data_imputed.reshape(T * N, F), schema=valid_features)\
    .drop('dwpf', 'feel')\
    .with_columns(pl.DataFrame([valid_station[i % len(valid_station)] for i in range(T*N)], schema={'station': pl.Categorical})\
          .join(pl.from_pandas(station_df),
                on='station',
                how='left'))\
    .select(pl.col(['station', 'lon', 'lat', 'elevation', 'tmpf', 'relh', 'sknt', 'drct_sin', 'drct_cos']))\
    .drop('lon', 'lat', 'elevation')\
    .with_columns(pl.col('relh')/100)

imputed_df = imputed_df.drop('tmpf', 'sknt')\
    .hstack(pl.DataFrame(sklearn.preprocessing.robust_scale(imputed_df.select(pl.col('tmpf'))), schema=['tmpf']))\
    .hstack(pl.DataFrame(sklearn.preprocessing.robust_scale(imputed_df.select(pl.col('sknt'))), schema=['sknt']))\
    .select(pl.col(['station', 'tmpf', 'relh', 'sknt', 'drct_sin', 'drct_cos']))

imputed_df
```

```{python, warning=FALSE, echo=FALSE}
imputed_pd_df = imputed_df.to_pandas()
imputed_pl_df = imputed_df.drop('station')
```

```{python, warning=FALSE, echo=FALSE}
row_count, feature_count = imputed_pl_df.shape
station_count = imputed_df.select(pl.col('station').unique()).count().item()
np_imp_array = imputed_pl_df.to_numpy().reshape(int(row_count/station_count), station_count, feature_count)
```

```{python, warning=FALSE, echo=FALSE}
def concat_past_timesteps(data, window=4):
    T, _, _ = data.shape

    slices = [data[i:T - window + i + 1] for i in range(window)]

    return np.concatenate(slices[::-1], axis=2)

lagged_array = concat_past_timesteps(np_imp_array, 28)
```

```{python, warning=FALSE, echo=FALSE}
T, N, F = lagged_array.shape

edge_index = [[i, j] for i in range(N) for j in range(N) if i != j]
edge_weight = np.array([float(adj[edge[0], edge[1]]) for edge in edge_index])
edge_index = np.array(edge_index).T
features = [lagged_array[t, :, :] for t in range(T-1)]
targets = [lagged_array[t+1, :, 0] for t in range(T-1)]
```

```{python, warning=FALSE, echo=FALSE}
class RecurrentGCN(torch.nn.Module):
    def __init__(self, node_features):
        super(RecurrentGCN, self).__init__()
        self.recurrent1 = torch_geometric_temporal.nn.recurrent.DCRNN(node_features, 64, 1)
        self.recurrent2 = torch_geometric_temporal.nn.recurrent.DCRNN(64, 32, 1)
        self.recurrent3 = torch_geometric_temporal.nn.recurrent.DCRNN(32, 32, 1)

        self.linear = torch.nn.Linear(32, 1)

    def forward(self, x, edge_index, edge_weight):
        h = self.recurrent1(x, edge_index, edge_weight)
        h = torch.nn.functional.relu(h)

        h = self.recurrent2(h, edge_index, edge_weight)
        h = torch.nn.functional.relu(h)

        h = self.recurrent3(h, edge_index, edge_weight)
        h = torch.nn.functional.relu(h)

        h = self.linear(h)
        return h
```

```{python, warning=FALSE, echo=FALSE}
dataset = torch_geometric_temporal.signal.StaticGraphTemporalSignal(
    edge_index=edge_index,
    edge_weight=edge_weight,
    features=features,
    targets=targets
)

train_dataset, test_dataset = torch_geometric_temporal.temporal_signal_split(dataset, train_ratio=(T-split_index)/T)
train_dataset, validation_dataset = torch_geometric_temporal.temporal_signal_split(train_dataset, train_ratio=0.8)

gnn_model = RecurrentGCN(node_features = F)

optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',
    factor=0.1,
    patience=5,
    min_lr=1e-5,
)
```

```{python, warning=FALSE, echo=FALSE}
if not os.path.exists('best_model.pt'):
    best_loss = float('inf')
    patience = 10
    trigger_times = 0
    target_epochs = 100

    gnn_model.train()
    with tqdm(total=target_epochs) as pbar:
        for epoch in range(target_epochs):
            gnn_model.train()
            torch.enable_grad()
            train_cost = 0

            for time, snapshot in enumerate(train_dataset):
                y_hat = gnn_model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
                train_cost += torch.mean((y_hat - snapshot.y)**2)

            train_cost /= (time + 1)
            train_cost.backward()
            optimizer.step()
            optimizer.zero_grad()

            scheduler.step(train_cost.item())

            gnn_model.eval()
            val_loss = 0
            with torch.no_grad():
                for time, snapshot in enumerate(validation_dataset):
                    y_hat = gnn_model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
                    val_loss += torch.mean((y_hat - snapshot.y)**2)
                val_loss /= (time + 1)

            pbar.set_postfix({
                'train_loss': train_cost.item(),
                'val_loss': val_loss.item(),
                'lr': optimizer.param_groups[0]['lr']
            })
            pbar.update(1)

            if best_loss < 0.05 and epoch > 1:
                print(f"Early stopping triggered at epoch {epoch + 1}")
                break

            if val_loss.item() < best_loss:
                best_loss = val_loss.item()
                trigger_times = 0
                torch.save(gnn_model.state_dict(), 'best_model.pt')
            else:
                trigger_times += 1
                if trigger_times >= patience:
                    print(f"Early stopping triggered at epoch {epoch + 1}")
                    break
else:
    gnn_model.load_state_dict(torch.load('best_model.pt'))
```

```{python, warning=FALSE, echo=FALSE}
gnn_model.eval()
cost = 0
for time, snapshot in enumerate(test_dataset):
    y_hat = gnn_model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)
    cost = cost + torch.mean((y_hat - snapshot.y)**2)
cost = cost / (time+1)
cost = cost.item()
print("MSE: {:.4f}".format(cost))
```

```{python, warning=FALSE, echo=FALSE}
all_preds = []
all_targets = []

gnn_model.eval()
with torch.no_grad():
    for snapshot in test_dataset:
        y_pred = gnn_model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).squeeze()
        y_true = snapshot.y.squeeze()
        all_preds.append(y_pred.cpu().numpy())
        all_targets.append(y_true.cpu().numpy())

all_preds = np.stack(all_preds)
all_targets = np.stack(all_targets)

num_nodes = all_preds.shape[1]
num_time_steps = all_preds.shape[0]

cols = 1
rows = num_nodes

fig, axes = plt.subplots(rows, cols, figsize=(15, 20), sharex=True, sharey=True)

for node in range(num_nodes):
    ax = axes[node]
    ax.plot(all_targets[:, node], label='Actual', color='blue', linewidth=1)
    ax.plot(all_preds[:, node], label='Predicted', color='orange', linewidth=1)
    ax.set_title(f'Station {stations_list[node]}')
    ax.grid(True)

# Only add legend to one plot
axes[0].legend(loc='upper right')

fig.suptitle("GNN Actual vs Predicted Over Time for Each Node", fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

```{python, warning=FALSE, echo=FALSE}
all_preds = []
all_targets = []

gnn_model.eval()
with torch.no_grad():
    for snapshot in test_dataset:
        y_pred = gnn_model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).squeeze()
        y_true = snapshot.y.squeeze()
        all_preds.append(y_pred.cpu().numpy())
        all_targets.append(y_true.cpu().numpy())

all_preds = np.stack(all_preds)
all_targets = np.stack(all_targets)

gnn_absolute_error = np.abs(np.subtract(np.array(all_preds, dtype=float), np.array(all_targets, dtype=float)))

fig, axes = plt.subplots(rows, cols, figsize=(15, 20), sharex=True, sharey=True)
for node in range(num_nodes):
    ax = axes[node]
    ax.plot(gnn_absolute_error[:, node], label='Error', color='blue', linewidth=1)
    ax.set_title(f'Station {stations_list[node]}')
    ax.grid(True)

axes[0].legend(loc='upper right')

fig.suptitle("GNN Absolute Error for Each Station", fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

```{python, warning=FALSE, echo=FALSE}
flattened_array = np.mean(lagged_array, axis=1)

T, N, F = lagged_array.shape

X = flattened_array[:-1]
X_train, X_test = np.split(X, [T-split_index])
y_true = flattened_array[1:, 0]
y_train, y_test = np.split(y_true, [T-split_index])

for node_index in range(N):
  node_slice = lagged_array[:, node_index, :]
  _, node_test = np.split(node_slice, [T-split_index])
```

```{python, warning=FALSE, echo=FALSE}
lr_model = sklearn.linear_model.LinearRegression()
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)

mse = sklearn.metrics.mean_squared_error(y_test, y_pred)
print(f"MSE: {mse:.4f}")
```

```{python, warning=FALSE, echo=FALSE}
num_nodes = lagged_array[T-split_index:, :, :].shape[1]
num_time_steps = lagged_array[T-split_index:, :, :].shape[0]

cols = 1
rows = num_nodes

fig, axes = plt.subplots(rows, cols, figsize=(15, 20), sharex=True, sharey=True)

for node in range(num_nodes):
    all_targets = lagged_array[T-split_index+1:, node, 0]
    all_preds = lr_model.predict(lagged_array[T-split_index:-1, node, :])
    ax = axes[node]
    ax.plot(all_targets, label='Actual', color='blue', linewidth=1)
    ax.plot(all_preds, label='Predicted', color='orange', linewidth=1)
    ax.set_title(f'Station {stations_list[node]}')
    ax.grid(True)

axes[0].legend(loc='upper right')

fig.suptitle("LR Actual vs Predicted Over Time for Each Node", fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

```{python, warning=FALSE, echo=FALSE}
num_nodes = lagged_array[T-split_index:, :, :].shape[1]
num_time_steps = lagged_array[T-split_index:, :, :].shape[0]

cols = 1
rows = num_nodes


fig, axes = plt.subplots(rows, cols, figsize=(15, 20), sharex=True, sharey=True)
for node in range(num_nodes):
    all_targets = lagged_array[T-split_index+1:, node, 0]
    all_preds = lr_model.predict(lagged_array[T-split_index:-1, node, :])
    lr_absolute_error = np.abs(np.subtract(all_targets, all_preds))

    ax = axes[node]
    ax.plot(lr_absolute_error, label='Error', color='blue', linewidth=1)
    ax.set_title(f'Station {stations_list[node]}')
    ax.grid(True)

axes[0].legend(loc='upper right')

fig.suptitle("LR Absolute Error for Each Station", fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

```{python, warning=FALSE, echo=FALSE}
num_nodes = lagged_array[T-split_index:, :, :].shape[1]
num_time_steps = lagged_array[T-split_index:, :, :].shape[0]

cols = 1
rows = num_nodes

all_preds = []
all_targets = []

gnn_model.eval()
with torch.no_grad():
    for snapshot in test_dataset:
        y_pred = gnn_model(snapshot.x, snapshot.edge_index, snapshot.edge_attr).squeeze()
        y_true = snapshot.y.squeeze()
        all_preds.append(y_pred.cpu().numpy())
        all_targets.append(y_true.cpu().numpy())

all_preds = np.stack(all_preds)
all_targets = np.stack(all_targets)

gnn_absolute_error = np.abs(np.subtract(np.array(all_preds, dtype=float), np.array(all_targets, dtype=float)))

fig, axes = plt.subplots(rows, 1, figsize=(15, 20), sharex=True, sharey=True)
for node in range(num_nodes):
    all_targets = lagged_array[T-split_index+1:, node, 0]
    all_preds = lr_model.predict(lagged_array[T-split_index:-1, node, :])
    lr_absolute_error = np.abs(np.subtract(all_targets, all_preds))
    relative_error = np.abs(gnn_absolute_error[1:, node] - lr_absolute_error)

    ax = axes[node]
    ax.plot(gnn_absolute_error[:, node], label='GNN - Error', color='blue', linewidth=1)
    ax.plot(lr_absolute_error, label='LR - Error', color='orange', linewidth=1)
    ax.set_title(f'Station {stations_list[node]}')
    ax.set_xlim(left=0, right=500)

axes[0].legend(loc='upper right')

fig.suptitle("GNN vs. LR Absolute Error for Each Station", fontsize=16)
fig.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}

```

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
